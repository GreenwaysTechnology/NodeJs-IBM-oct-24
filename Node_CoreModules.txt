.....................................................................................
			Types of modules
.....................................................................................

1.Custom module
  built by us
2.built in modules
   provided by node.js  
3.provided by third party/community
  libs,frameworks

Built in Node.js Modules:
.........................
https://nodejs.org/docs/latest/api/

Built in node modules provides

1.non blocking networking io apis
   -tcp,http,datagram etc...
2.timers
3.file system api
   to read and write data into disk files
4.common apis
   os,events

Common apis:

1.OS
const os = require('node:os')

function main(){
    console.log(os.arch())
    console.log(os.cpus())
}
main()

./ vs ''
.........

 require('./services/TODOService');
  ->here you can see ./
  ./ -current dir

 require('node:os'); => 
  -here no ./ 

Why?

Note : if you are java devp, you know the classpath , how it works?

require('node:os');

Node internally uses a search algorthim,node always looks the folder called
 "node_modules" in the current project, if not , then it searches, the node in built 
installtion folder---c:/pf/node/node_modules--if it finds it will pick up from there else it will throw error.

require('./services/TODOService');
   it will lookup in the current dir or sub dirs only.


.....................................................................................
.....................................................................................
				  Events

Node.js is event driven arch.

There are there two types of events

1.OS kernal events
   -timer (setTimeout,setInterval)
   -IO events -  network,fs

2.Application events
   -it could be any thing.
   One object can send message to another object via events.
   Objects sends message over some message channel.

Events moudle  is core module in node, which can be used to kernals events as well as application.

Much of the Node.js core API is built around an idiomatic asynchronous event-driven architecture in which certain kinds of objects (called "emitters") emit named events that cause Function objects ("listeners") to be called.

Emitter is kernal or Application object, Listener is a function which is otherwise called as callbacks.
  
Using events module, we can send and receive messages within single system or across sytem( microservice)

const EventEmitter = require('node:events')

class Sales extends EventEmitter {
    constructor() {
        super()
        //register listener 
        this.on('sold', (evt) => {
            console.log(evt)
        })
    }
    //biz method
    sell(product) {
        //emit event
        this.emit('sold', product)
    }
}

function main() {
    let sales = new Sales()
    sales.sell({ id: 1, name: 'Iphone', price: 100000 })

}
main()
....................................................................................
			 IO modules

1.File system IO
2.HTTP IO - Web Programming


File System IO:
=>We can read and write files from the disk in  two ways
  1.blocking way
  2.nonblocking way
=>We can read and write files using two mode (nonblocking apis)
  1.NonStreaming mode
  2.Streaming mode


=>All file operations are handled by
  "Worker Threads" from Worker Thread Pool - either it is blocking or non blocking    io.
=>Files are handled using callback style or promise style.
=>Files operations are handled by "fs" module


How to read File using nonblocking pattern? using callbacks

fs.readFile(path[, options], callback)

path <string> | <Buffer> | <URL> | <integer> filename or file descriptor
options <Object> | <string>
 encoding <string> | <null> Default: null
 flag <string> See support of file system flags. Default: 'r'.
 signal <AbortSignal> allows aborting an in-progress readFile

callback <Function>
  err <Error> | <AggregateError>
  data <string> | <Buffer>

.....................................................................................

eg:
Use Case : read file using callbacks

const fs = require('node:fs');

const fs = require('node:fs');

//read

function readFile() {
    const filePath = './src/assets/info.txt'
    const options = {
        encoding: 'UTF-8'
    }
    fs.readFile(filePath, options, (err, data) => {
        if (err) throw err
        console.log(data)
    })

}

function main() {
    console.log('start')
    readFile()
    console.log('end')
}
main()


Task:
 The above code is based on callback, convert this into promise.
 Note: You have to write your own promise Implementation

Solution:
// const fs = require('node:fs')
const { readFile } = require('node:fs')


function getFile() {
    return new Promise((resolve, reject) => {
        const path = './src/assets/info.txt'
        const options = {
            encoding: 'utf-8'
        }
        readFile(path, options, (err, data) => {
            if (err) {
                reject(err)
            } else {
                resolve(data)
            }
        })
    })
}


// function main() {
//     getFile().then(data => console.log(data)).catch(err => console.log(err))
// }
async function main() {
    try {
        const data = await getFile()
        console.log(data)
    }
    catch (err) {
        console.log(err)
    }
}
main()
.....................................................................................

Read file using inbuilt promise package 

const fs = require('node:fs/promises');



async function readFile() {
    const filePath = './src/assets/info.txt'
    const options = {
        encoding: 'UTF-8'
    }
    try {
        const data = await fs.readFile(filePath, options)
        console.log(data)
    }
    catch (err) {
        console.log(err)
    }

}

function main() {
    readFile()
}
main()

.....................................................................................
			Mode of fs read and write
.....................................................................................

1.Non Streaming Mode

2.Streaming  Mode

1.Non Streaming Mode

  only file io is supported, network io not supported

 -once file is read, the entire file is loaded into node process buffer(memory), then it will be delivered to caller.

-if more files are loaded into node process, node process gets crashed.

-non streaming mode is not suitable for large and big files read or write operation.

fs.readFile() and fs.writeFile are non streaming apis

2.Streaming apis:
   supported by fs and also network apis

-Streaming is nothing but flow of data(chunks).
-Streaming allows move the data from one place to another place one by one.
-Streaming apis are other wise called evented io. which is powered events.

Types of Streams:

1.Readable Stream : input
2.Writeable stream : output
3.Duplex stream : read + write

Node has lot of built in stream apis
....................................

Built in readable Streams:

-HTTP responses, on the client
-HTTP requests, on the server
-fs read streams
-zlib streams
-crypto streams
-TCP sockets
-child process stdout and stderr
-process.stdin

Writable Streams:

-HTTP requests, on the client
-HTTP responses, on the server
-fs write streams
-zlib streams
-crypto streams
-TCP sockets
-child process stdin
-process.stdout, process.stderr

All streaming apis are powered with events
node io streams has built in events.
events are emitted by node.
Our programs are listeners

Common events in all io
.........................


1.data event:
 which is emitted by node, for each chunk.

2.close event:
  The 'close' event is emitted when the stream and any of its underlying resources (a file descriptor, for example) have been closed.

3.end event:
 The 'end' event is emitted when there is no more data to be consumed from the stream.

4.Event: 'error'
 The 'error' event may be emitted by a Readable implementation at any time
Typically, this may occur if the underlying stream is unable to generate data due to an underlying internal failure, or when a stream implementation attempts to push an invalid chunk of data.


const fs = require('node:fs')

function main() {
    const filePath = './src/assets/info.txt'
    const options = {
        encoding: 'UTF-8'
    }
    //create input stream 
    const inputstream = fs.createReadStream(filePath, options)

    //attach listener
    let data = ''
    inputstream.on('data', (chunk) => {
        data += chunk
    })
    inputstream.on('end', () => {
        console.log(data)
    })
    inputstream.on('close', () => {
        console.log('close event is called')
    })
    inputstream.on('error', (err) => {
	consoe.log(err)
        console.log('error event is called')
    })
}
main()

....................................................................................
			   path module
...................................................................................

path module provides abstraction for paths and dirs structure in platform independant way.

Global variables:

 node provides some global variables for dealing with path.

 __dirname  -  get current full path


path.join:
  It helps to take current working dir.

const fs = require('node:fs')
const path = require('node:path')

function main() {
    const filePath = path.join(__dirname, 'assets/info.txt')
    const options = {
        encoding: 'UTF-8'
    }
    //create input stream 
    const inputstream = fs.createReadStream(filePath, options)

    //attach listener
    let data = ''
    inputstream.on('data', (chunk) => {
        data += chunk
    })
    inputstream.on('end', () => {
        console.log(data)
    })
    inputstream.on('close', () => {
        console.log('close event is called')
    })
    inputstream.on('error', (err) => {
        consoe.log(err)
        console.log('error event is called')
    })
}
main()
...................................................................................
			 Writing file into streams
...................................................................................

const fs = require('fs');
const path = require('path');

function main() {
    const fileName = path.join(__dirname, 'assets/courses.txt');
    const config = {
        encoding: 'utf8',
        flag: 'w'
    };
    const outputStream = fs.createWriteStream(fileName, config);
    const data = ['react', 'node', 'microservices', 'devops'];

    data.forEach((data)=>{
        outputStream.write(data + " ");
        console.log("Wrote: %s", data);
    })

    outputStream.close();

    outputStream.on('close', function () {
        console.log('file has been closed ')
    })
}
main()

.....................................................................................
				Back Pressure
....................................................................................

When input stream and output stream works together.

Backpressure:
Problems when you do read and write together

1. In general read operation is faster than write operation


Back Pressure means inputstream is fast, outputstream slow, then data will be
lost.


How to handle back pressure?

 apis  : pause,resume,drain event

pause : to close the upstream, not to emit data
resume : to open the open upstream , to emit data

drain event: if drain event is called, means buffer is empty

In order test back pressure , we need atleast 1gb file.



Create big file code:
//big file creation
const fs = require('fs');
const path = require('path')

const filePath = path.join(__dirname, "assets/big.file")

const file = fs.createWriteStream(filePath);

for (let i = 0; i <= 1e6; i++) {
    file.write('Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n');
}

file.end();

..

Back pressure handling code:
............................
const fs = require('node:fs');
const path = require('node:path');

const inputfileName = path.join(__dirname, 'assets/big.file');
const outputfileName = path.join(__dirname, 'assets/bigcopy.file');

const config = {
    encoding: 'UTF-8'
}
const readerStream = fs.createReadStream(inputfileName, config);
const writeStr = fs.createWriteStream(outputfileName, config);

readerStream.on('data', function (chunk) {
    console.log(`Received ${chunk.length} bytes of data.`);
    let buffer_good = writeStr.write(chunk);
    if (!buffer_good) readerStream.pause();
});
writeStr.on('drain', function () {
    console.log('buffer drained!');
    readerStream.resume();
});
readerStream.on('end', function () {
     //here you can process data.
});

readerStream.on('error', function (err) {
    console.log(err.stack);
});

.....................................................................................
			Backpressure handling using pipes
.....................................................................................
const fs = require('node:fs');
const path = require('node:path');

const inputfileName = path.join(__dirname, 'assets/big.file');
//write
const outputFileName = path.join(__dirname, 'assets/bigcopy.file');

const config = {
      encoding: 'UTF-8'
}

//Back pressure handling
const readerStream = fs.createReadStream(inputfileName, config);
const writeStr = fs.createWriteStream(outputFileName, config);

//backPressure streams
//pipe method is simplest method which wraps resume,pasuse,drain 
readerStream.pipe(writeStr);






